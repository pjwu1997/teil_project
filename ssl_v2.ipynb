{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T08:34:16.681349Z",
     "start_time": "2021-05-04T08:34:15.898421Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import pickle\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T08:34:52.559466Z",
     "start_time": "2021-05-04T08:34:51.789129Z"
    },
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12000 unlabeled data\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "labeled_portion = 0.8\n",
    "train_portion = 0.8\n",
    "full_train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(),  download=True)\n",
    "\n",
    "labeled_size = int(labeled_portion * len(full_train_dataset))\n",
    "unlabeled_size = len(full_train_dataset) - labeled_size\n",
    "labeled_data, unlabeled_data = torch.utils.data.random_split(full_train_dataset, [labeled_size, unlabeled_size])\n",
    "\n",
    "train_size = int(train_portion * len(labeled_data))\n",
    "val_size = len(labeled_data) - train_size\n",
    "train_data, val_data = torch.utils.data.random_split(labeled_data, [train_size, val_size])\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "# Here we take out all unlabeled data and ignore the labels\n",
    "unlabeled_data = [unlabeled_data[i][0] for i in range(len(unlabeled_data))]\n",
    "\n",
    "# Print the number of unlabeled data\n",
    "print(f'There are {len(unlabeled_data)} unlabeled data')\n",
    "\n",
    "# create dataset\n",
    "class labeled_dataset(Dataset):\n",
    "    def __init__(self, labeled_data, labels):\n",
    "        self.samples = labeled_data\n",
    "        self.labels = labels\n",
    "        self.n_samples = len(labeled_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # print(self.samples[index].unsqueeze(0).shape)\n",
    "        return self.samples[index], self.labels[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "class unlabeled_datasest(Dataset):\n",
    "    def __init__(self, unlabeled_data):\n",
    "        self.samples = labeled_data\n",
    "        self.n_samples = len(new_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # print(self.samples[index].unsqueeze(0).shape)\n",
    "        return self.samples[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "unlabeled_loader = torch.utils.data.DataLoader(unlabeled_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T08:34:54.225030Z",
     "start_time": "2021-05-04T08:34:52.560617Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FC1(\n",
       "  (shrink): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=300, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=300, out_features=10, bias=True)\n",
       "  )\n",
       "  (expand): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=300, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=300, out_features=784, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model setting\n",
    "\n",
    "# Hyperparameters\n",
    "in_dim = 784\n",
    "out_dim = 10 \n",
    "hid_dim = 300\n",
    "n_epoch = 5\n",
    "lr = 1e-4\n",
    "\n",
    "MAX_ESC = 10\n",
    "class FC1(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, hid_dim, out_dim):\n",
    "        super(FC1, self).__init__()\n",
    "        self.shrink = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_dim, hid_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hid_dim, out_dim)\n",
    "            )\n",
    "        self.expand = torch.nn.Sequential(\n",
    "            torch.nn.Linear(out_dim, hid_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hid_dim, in_dim)\n",
    "            )\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.shrink(x)\n",
    "        return x\n",
    "\n",
    "# model = torch.nn.Sequential(\n",
    "#     torch.nn.Linear(in_dim, hid_dim),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(hid_dim, out_dim),\n",
    "# )\n",
    "model = FC1(in_dim, hid_dim, out_dim)\n",
    "loss_fcn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# print(model)\n",
    "# print(next(model.parameters()).device)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T08:34:54.236069Z",
     "start_time": "2021-05-04T08:34:54.226283Z"
    }
   },
   "outputs": [],
   "source": [
    "#----------------#\n",
    "# Initialization #\n",
    "#----------------#\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "def train(self):\n",
    "    ############\n",
    "    # Training #\n",
    "    ############\n",
    "\n",
    "    n_batch = len(train_loader)\n",
    "    best_val_acc = 0\n",
    "    esc = 0\n",
    "    val_acc = 0\n",
    "\n",
    "    #----------------#\n",
    "    # Start training #\n",
    "    #----------------#\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        total_cnt, correct_cnt, train_loss, total_loss = 0, 0, 0, 0\n",
    "        \n",
    "        for batch, (images, labels) in enumerate(train_loader):\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "            \n",
    "            predictions = model(images.view(-1, in_dim))\n",
    "            loss = loss_fcn(predictions, labels)\n",
    "        \n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate the training loss and accuracy of each iteration\n",
    "            _, pred_labels = torch.max(predictions, 1)\n",
    "            total_cnt += images.size(0)\n",
    "            correct_cnt += (pred_labels == labels).sum().item()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Show the training information\n",
    "            # if batch % 100 == 0 or batch == len(train_loader):\n",
    "            #     acc = correct_cnt / total_cnt\n",
    "            #     print(\n",
    "            #         f\"Epoch [{epoch+1}/{n_epoch}], Step [{batch}/{n_batch}], Train loss: {loss.item():.6f}, Train acc: {acc * 100:.3f} %\")\n",
    "            acc = correct_cnt / total_cnt\n",
    "            print(f\"\\rEpoch [{epoch+1}/{n_epoch}], Step [{batch:3}/{n_batch}], Train loss: {loss.item():.6f}, Train acc: {acc * 100:.3f} % val_acc: {val_acc * 100:.3f} %, {esc} / {MAX_ESC}\", end='')\n",
    "        #------------#\n",
    "        # Validating #\n",
    "        #------------#\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():  # No need BP\n",
    "            total_cnt, correct_cnt, val_loss = 0, 0, 0\n",
    "            \n",
    "            for batch, (images, labels) in enumerate(val_loader, 1):\n",
    "                \n",
    "                # Put input tensor to GPU if it's available\n",
    "                if torch.cuda.is_available():\n",
    "                    images = images.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    # images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "                # Forward pass\n",
    "                predictions = model(images.view(-1, in_dim))\n",
    "                loss = loss_fcn(predictions, labels)\n",
    "                \n",
    "                # Calculate the training loss and accuracy of each iteration\n",
    "                _, pred_labels = torch.max(predictions, 1)\n",
    "                total_cnt += images.size(0)\n",
    "                correct_cnt += (pred_labels == labels).sum().item()\n",
    "                val_loss += loss.item()\n",
    "\n",
    "            val_acc = correct_cnt / total_cnt\n",
    "            # print(f\"\\rval_acc: {val_acc * 100:.3f} %, {esc} / {MAX_ESC}\", end='')\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "\n",
    "                # Save trained model\n",
    "                torch.save(model.state_dict(), f\"./checkpoint/NN.pth\" )\n",
    "                # print('(model updated!)')\n",
    "                esc = 0\n",
    "            else:\n",
    "#                 print('(model dropped)')\n",
    "                esc += 1\n",
    "\n",
    "            \n",
    "        if esc > MAX_ESC:\n",
    "            break\n",
    "\n",
    "    \n",
    "    \n",
    "    print('\\nFinish training')\n",
    "    print(f'Best val acc = {best_val_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T08:34:55.369375Z",
     "start_time": "2021-05-04T08:34:55.354140Z"
    }
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# Pseudo-labeling by rank #\n",
    "###########################\n",
    "\n",
    "top = 50\n",
    "\n",
    "def pseudoLabel(self):\n",
    "\n",
    "    global train_loader\n",
    "\n",
    "    pseudo_labels = []\n",
    "    all_confidence = []\n",
    "    pred_list = []\n",
    "    img_list = []\n",
    "\n",
    "    print('Getting pseudolabels', end=': ')\n",
    "    with torch.no_grad(): # No need BP\n",
    "        \n",
    "        # Record variables and containers\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        n_class_correct = [0] * 10\n",
    "        n_class_samples = [0] * 10\n",
    "        \n",
    "        # top = itertools.islice(unlabeled_loader, 5)\n",
    "        for images in unlabeled_loader:\n",
    "            \n",
    "            # Get the GPU support\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.to(device)\n",
    "            \n",
    "            # Predict via forward pass\n",
    "            predictions = model(images.view(-1, 784))\n",
    "            \n",
    "            confidence, predicted_labels = torch.max(predictions, dim=1)\n",
    "            all_confidence += confidence\n",
    "            pred_list += predicted_labels\n",
    "            img_list += images\n",
    "    print('done')\n",
    "\n",
    "    print(f'Finding the top {top}', end=': ')\n",
    "    top_c = np.argpartition(all_confidence, -top)[-top:]\n",
    "    pseudo_labels = np.array(pred_list)[top_c]\n",
    "    confident_unlabeled = np.array(img_list)[top_c]\n",
    "    print('done')\n",
    "\n",
    "\n",
    "    print('Updating U-data', end=': ')\n",
    "    all_indices = np.arange(len(unlabeled_data))\n",
    "    remain_indices = np.delete(all_indices, top_c)\n",
    "    unlabeled_data[:] = [unlabeled_data[i] for i in remain_indices]\n",
    "    print('done')\n",
    "\n",
    "    print('Updating L-data', end=': ')\n",
    "    original_dataset = None\n",
    "    original_labels = None\n",
    "\n",
    "    if os.path.isfile('x.pkl') is False or os.path.isfile('y.pkl') is False:\n",
    "        for images, labels in train_loader:\n",
    "            if original_dataset is None:\n",
    "                original_dataset = images\n",
    "            else:\n",
    "                original_dataset = torch.cat((original_dataset, images))\n",
    "            \n",
    "            if original_labels is None:\n",
    "                original_labels = labels\n",
    "            else:\n",
    "                original_labels = torch.cat((original_labels, labels))\n",
    "\n",
    "    with open('x.pkl', 'rb') as handle:\n",
    "        original_dataset = pickle.load(handle)\n",
    "    with open('y.pkl', 'rb') as handle:\n",
    "        original_labels = pickle.load(handle)\n",
    "\n",
    "    confident_unlabeled = torch.stack(list(confident_unlabeled))\n",
    "    pseudo_labels = torch.stack(list(pseudo_labels))\n",
    "    new_dataset = torch.cat((original_dataset, confident_unlabeled.cpu()))\n",
    "    new_labels = torch.cat((original_labels, pseudo_labels.cpu()))\n",
    "    train_data = labeled_dataset(new_dataset, new_labels)    \n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # print(len(train_loader))\n",
    "    \n",
    "    with open('x.pkl', 'wb') as handle:\n",
    "        pickle.dump(new_dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('y.pkl', 'wb') as handle:\n",
    "        pickle.dump(new_labels, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print('done')\n",
    "    \n",
    "    # print(f'Accepting rate: {len(confident_unlabeled) / len(unlabeled_data) * 100:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T08:34:57.670822Z",
     "start_time": "2021-05-04T08:34:57.664168Z"
    }
   },
   "outputs": [],
   "source": [
    "class SSL():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.labeled_data = labeled_data\n",
    "    \n",
    "    def describe(self):\n",
    "        pass\n",
    "        # describe data and model\n",
    "\n",
    "    train = train\n",
    "    \n",
    "    def validate(self):\n",
    "        pass\n",
    "        # todo\n",
    "    \n",
    "    def test(self):\n",
    "        pass\n",
    "        # todo\n",
    "    \n",
    "    pseudoLabel = pseudoLabel\n",
    "    \n",
    "    def selection(self):\n",
    "        pass\n",
    "        # todo\n",
    "    \n",
    "    def oneRun(self):\n",
    "        self.train()\n",
    "        self.pseudoLabel()\n",
    "        # todo\n",
    "\n",
    "    def shouldTerminate(self):\n",
    "        pass\n",
    "        # todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T08:34:59.948050Z",
     "start_time": "2021-05-04T08:34:59.942481Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ssl = SSL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-04T08:35:01.375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Round 0 ---\n",
      "Epoch [5/5], Step [599/600], Train loss: 2.090021, Train acc: 26.505 % val_acc: 22.302 %, 0 / 10\n",
      "Finish training\n",
      "Best val acc = 0.3103125\n",
      "Getting pseudolabels: done\n",
      "Finding the top 50: done\n",
      "Updating U-data: done\n",
      "Updating L-data: done\n",
      "--- Round 1 ---\n",
      "Epoch [5/5], Step [613/614], Train loss: 1.812702, Train acc: 57.646 % val_acc: 54.844 %, 0 / 10\n",
      "Finish training\n",
      "Best val acc = 0.5827083333333334\n",
      "Getting pseudolabels: done\n",
      "Finding the top 50: done\n",
      "Updating U-data: done\n",
      "Updating L-data: done\n",
      "--- Round 2 ---\n",
      "Epoch [5/5], Step [614/615], Train loss: 1.469839, Train acc: 68.855 % val_acc: 67.312 %, 0 / 10\n",
      "Finish training\n",
      "Best val acc = 0.6859375\n",
      "Getting pseudolabels: done\n",
      "Finding the top 50: done\n",
      "Updating U-data: done\n",
      "Updating L-data: done\n",
      "--- Round 3 ---\n",
      "Epoch [4/5], Step [614/615], Train loss: 1.396303, Train acc: 73.337 % val_acc: 71.740 %, 0 / 10"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(f'--- Round {i} ---')\n",
    "    ssl.oneRun()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
