{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python373jvsc74a57bd04d2609df99acd88c1623c81f3d9d6d6781319438fb694fe1124a29d0cf557947",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "in_dim = 784\n",
    "out_dim = 10 \n",
    "hid_dim = 300\n",
    "\n",
    "n_epoch = 20\n",
    "lr = 1e-4\n",
    "batch_size = 64\n",
    "MAX_ESC = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "48000\nTrain data\t38400\nUnabeled data\t12000\nVal data\t9600\nTest data\t10000\n"
     ]
    }
   ],
   "source": [
    "#################\n",
    "# MNIST dataset #\n",
    "#################\n",
    "\n",
    "#--------------------#\n",
    "# Data preprocessing #\n",
    "#--------------------#\n",
    "'''\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "'''\n",
    "\n",
    "labeled_portion = 0.8\n",
    "train_portion = 0.8\n",
    "\n",
    "#------------------------#\n",
    "# Load and split dataset #\n",
    "#------------------------#\n",
    "\n",
    "full_train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(),  download=True)\n",
    "\n",
    "labeled_size = int(labeled_portion * len(full_train_dataset))\n",
    "unlabeled_size = len(full_train_dataset) - labeled_size\n",
    "labeled_data, unlabeled_data = torch.utils.data.random_split(full_train_dataset, [labeled_size, unlabeled_size])\n",
    "\n",
    "train_size = int(train_portion * len(labeled_data))\n",
    "val_size = len(labeled_data) - train_size\n",
    "train_data, val_data = torch.utils.data.random_split(labeled_data, [train_size, val_size])\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "#------------------#\n",
    "# Build dataloader #\n",
    "#------------------#\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "unlabeled_loader = torch.utils.data.DataLoader(unlabeled_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#-------------------#\n",
    "# Show dataset info #\n",
    "#-------------------#\n",
    "\n",
    "print(f'Train data\\t{len(train_data)}')\n",
    "print(f'Unabeled data\\t{len(unlabeled_data)}')\n",
    "print(f'Val data\\t{len(val_data)}')\n",
    "print(f'Test data\\t{len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=300, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=300, out_features=10, bias=True)\n",
      ")\n",
      "cpu\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#########\n",
    "# Model #\n",
    "#########\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_dim, hid_dim),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hid_dim, out_dim),\n",
    ")\n",
    "\n",
    "loss_fcn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# print(model)\n",
    "# print(next(model.parameters()).device)\n",
    "model.cuda()\n",
    "# print(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch [1/20], Step [0/600], Train loss: 2.427868, Train acc: 7.812 %\n",
      "Epoch [1/20], Step [100/600], Train loss: 2.401922, Train acc: 6.745 %\n",
      "Epoch [1/20], Step [200/600], Train loss: 2.476192, Train acc: 6.600 %\n",
      "Epoch [1/20], Step [300/600], Train loss: 2.475604, Train acc: 6.722 %\n",
      "Epoch [1/20], Step [400/600], Train loss: 2.394784, Train acc: 6.940 %\n",
      "Epoch [1/20], Step [500/600], Train loss: 2.359475, Train acc: 7.148 %\n",
      "val_acc: 8.438 %, 0 / 10 (model updated!)\n",
      "Epoch [2/20], Step [0/600], Train loss: 2.331833, Train acc: 9.375 %\n",
      "Epoch [2/20], Step [100/600], Train loss: 2.409046, Train acc: 9.452 %\n",
      "Epoch [2/20], Step [200/600], Train loss: 2.419545, Train acc: 9.530 %\n",
      "Epoch [2/20], Step [300/600], Train loss: 2.318299, Train acc: 9.463 %\n",
      "Epoch [2/20], Step [400/600], Train loss: 2.334238, Train acc: 9.691 %\n",
      "Epoch [2/20], Step [500/600], Train loss: 2.249120, Train acc: 9.952 %\n",
      "val_acc: 12.146 %, 0 / 10 (model updated!)\n",
      "Epoch [3/20], Step [0/600], Train loss: 2.397802, Train acc: 9.375 %\n",
      "Epoch [3/20], Step [100/600], Train loss: 2.341503, Train acc: 13.057 %\n",
      "Epoch [3/20], Step [200/600], Train loss: 2.186677, Train acc: 12.896 %\n",
      "Epoch [3/20], Step [300/600], Train loss: 2.268489, Train acc: 13.414 %\n",
      "Epoch [3/20], Step [400/600], Train loss: 2.371869, Train acc: 13.731 %\n",
      "Epoch [3/20], Step [500/600], Train loss: 2.203173, Train acc: 14.159 %\n",
      "val_acc: 17.708 %, 0 / 10 (model updated!)\n",
      "Epoch [4/20], Step [0/600], Train loss: 2.204381, Train acc: 18.750 %\n",
      "Epoch [4/20], Step [100/600], Train loss: 2.139630, Train acc: 17.713 %\n",
      "Epoch [4/20], Step [200/600], Train loss: 2.192660, Train acc: 18.424 %\n",
      "Epoch [4/20], Step [300/600], Train loss: 2.255582, Train acc: 18.745 %\n",
      "Epoch [4/20], Step [400/600], Train loss: 2.210900, Train acc: 19.299 %\n",
      "Epoch [4/20], Step [500/600], Train loss: 2.218484, Train acc: 19.838 %\n",
      "val_acc: 23.948 %, 0 / 10 (model updated!)\n",
      "Epoch [5/20], Step [0/600], Train loss: 2.096302, Train acc: 32.812 %\n",
      "Epoch [5/20], Step [100/600], Train loss: 2.189010, Train acc: 24.304 %\n",
      "Epoch [5/20], Step [200/600], Train loss: 2.134350, Train acc: 25.249 %\n",
      "Epoch [5/20], Step [300/600], Train loss: 2.086767, Train acc: 25.810 %\n",
      "Epoch [5/20], Step [400/600], Train loss: 2.139694, Train acc: 26.422 %\n",
      "Epoch [5/20], Step [500/600], Train loss: 2.053982, Train acc: 26.803 %\n",
      "val_acc: 30.635 %, 0 / 10 (model updated!)\n",
      "Epoch [6/20], Step [0/600], Train loss: 2.042713, Train acc: 40.625 %\n",
      "Epoch [6/20], Step [100/600], Train loss: 1.981269, Train acc: 31.281 %\n",
      "Epoch [6/20], Step [200/600], Train loss: 2.023099, Train acc: 31.561 %\n",
      "Epoch [6/20], Step [300/600], Train loss: 2.084412, Train acc: 32.018 %\n",
      "Epoch [6/20], Step [400/600], Train loss: 2.032119, Train acc: 32.742 %\n",
      "Epoch [6/20], Step [500/600], Train loss: 2.070761, Train acc: 32.984 %\n",
      "val_acc: 37.146 %, 0 / 10 (model updated!)\n",
      "Epoch [7/20], Step [0/600], Train loss: 2.011010, Train acc: 43.750 %\n",
      "Epoch [7/20], Step [100/600], Train loss: 1.965724, Train acc: 37.438 %\n",
      "Epoch [7/20], Step [200/600], Train loss: 1.959654, Train acc: 37.826 %\n",
      "Epoch [7/20], Step [300/600], Train loss: 2.039356, Train acc: 38.190 %\n",
      "Epoch [7/20], Step [400/600], Train loss: 1.966339, Train acc: 38.837 %\n",
      "Epoch [7/20], Step [500/600], Train loss: 2.014096, Train acc: 39.490 %\n",
      "val_acc: 42.927 %, 0 / 10 (model updated!)\n",
      "Epoch [8/20], Step [0/600], Train loss: 1.933508, Train acc: 48.438 %\n",
      "Epoch [8/20], Step [100/600], Train loss: 1.932144, Train acc: 43.688 %\n",
      "Epoch [8/20], Step [200/600], Train loss: 1.921674, Train acc: 44.349 %\n",
      "Epoch [8/20], Step [300/600], Train loss: 1.977855, Train acc: 44.440 %\n",
      "Epoch [8/20], Step [400/600], Train loss: 1.978536, Train acc: 45.048 %\n",
      "Epoch [8/20], Step [500/600], Train loss: 1.837673, Train acc: 45.375 %\n",
      "val_acc: 48.948 %, 0 / 10 (model updated!)\n",
      "Epoch [9/20], Step [0/600], Train loss: 1.930757, Train acc: 50.000 %\n",
      "Epoch [9/20], Step [100/600], Train loss: 1.910667, Train acc: 50.046 %\n",
      "Epoch [9/20], Step [200/600], Train loss: 1.903906, Train acc: 50.225 %\n",
      "Epoch [9/20], Step [300/600], Train loss: 1.936877, Train acc: 50.135 %\n",
      "Epoch [9/20], Step [400/600], Train loss: 1.830853, Train acc: 50.538 %\n",
      "Epoch [9/20], Step [500/600], Train loss: 1.852806, Train acc: 50.917 %\n",
      "val_acc: 53.927 %, 0 / 10 (model updated!)\n",
      "Epoch [10/20], Step [0/600], Train loss: 1.848683, Train acc: 56.250 %\n",
      "Epoch [10/20], Step [100/600], Train loss: 1.806197, Train acc: 53.728 %\n",
      "Epoch [10/20], Step [200/600], Train loss: 1.796225, Train acc: 54.198 %\n",
      "Epoch [10/20], Step [300/600], Train loss: 1.885917, Train acc: 54.662 %\n",
      "Epoch [10/20], Step [400/600], Train loss: 1.798721, Train acc: 55.046 %\n",
      "Epoch [10/20], Step [500/600], Train loss: 1.831780, Train acc: 55.436 %\n",
      "val_acc: 57.563 %, 0 / 10 (model updated!)\n",
      "Epoch [11/20], Step [0/600], Train loss: 1.822477, Train acc: 67.188 %\n",
      "Epoch [11/20], Step [100/600], Train loss: 1.822614, Train acc: 58.648 %\n",
      "Epoch [11/20], Step [200/600], Train loss: 1.822193, Train acc: 58.823 %\n",
      "Epoch [11/20], Step [300/600], Train loss: 1.789995, Train acc: 58.861 %\n",
      "Epoch [11/20], Step [400/600], Train loss: 1.869345, Train acc: 59.145 %\n",
      "Epoch [11/20], Step [500/600], Train loss: 1.726802, Train acc: 59.172 %\n",
      "val_acc: 60.948 %, 0 / 10 (model updated!)\n",
      "Epoch [12/20], Step [0/600], Train loss: 1.706227, Train acc: 70.312 %\n",
      "Epoch [12/20], Step [100/600], Train loss: 1.741704, Train acc: 61.402 %\n",
      "Epoch [12/20], Step [200/600], Train loss: 1.807668, Train acc: 61.567 %\n",
      "Epoch [12/20], Step [300/600], Train loss: 1.657182, Train acc: 61.706 %\n",
      "Epoch [12/20], Step [400/600], Train loss: 1.724003, Train acc: 61.803 %\n",
      "Epoch [12/20], Step [500/600], Train loss: 1.774701, Train acc: 62.144 %\n",
      "val_acc: 63.656 %, 0 / 10 (model updated!)\n",
      "Epoch [13/20], Step [0/600], Train loss: 1.658824, Train acc: 76.562 %\n",
      "Epoch [13/20], Step [100/600], Train loss: 1.740994, Train acc: 64.155 %\n",
      "Epoch [13/20], Step [200/600], Train loss: 1.629140, Train acc: 63.853 %\n",
      "Epoch [13/20], Step [300/600], Train loss: 1.679866, Train acc: 64.057 %\n",
      "Epoch [13/20], Step [400/600], Train loss: 1.717204, Train acc: 64.129 %\n",
      "Epoch [13/20], Step [500/600], Train loss: 1.813262, Train acc: 64.228 %\n",
      "val_acc: 65.792 %, 0 / 10 (model updated!)\n",
      "Epoch [14/20], Step [0/600], Train loss: 1.626091, Train acc: 70.312 %\n",
      "Epoch [14/20], Step [100/600], Train loss: 1.658028, Train acc: 65.161 %\n",
      "Epoch [14/20], Step [200/600], Train loss: 1.800930, Train acc: 64.871 %\n",
      "Epoch [14/20], Step [300/600], Train loss: 1.557757, Train acc: 65.750 %\n",
      "Epoch [14/20], Step [400/600], Train loss: 1.541966, Train acc: 65.867 %\n",
      "Epoch [14/20], Step [500/600], Train loss: 1.663192, Train acc: 66.108 %\n",
      "val_acc: 67.344 %, 0 / 10 (model updated!)\n",
      "Epoch [15/20], Step [0/600], Train loss: 1.649801, Train acc: 64.062 %\n",
      "Epoch [15/20], Step [100/600], Train loss: 1.736239, Train acc: 66.631 %\n",
      "Epoch [15/20], Step [200/600], Train loss: 1.638948, Train acc: 67.436 %\n",
      "Epoch [15/20], Step [300/600], Train loss: 1.604009, Train acc: 67.608 %\n",
      "Epoch [15/20], Step [400/600], Train loss: 1.681726, Train acc: 67.421 %\n",
      "Epoch [15/20], Step [500/600], Train loss: 1.598902, Train acc: 67.612 %\n",
      "val_acc: 68.865 %, 0 / 10 (model updated!)\n",
      "Epoch [16/20], Step [0/600], Train loss: 1.610598, Train acc: 70.312 %\n",
      "Epoch [16/20], Step [100/600], Train loss: 1.579921, Train acc: 68.858 %\n",
      "Epoch [16/20], Step [200/600], Train loss: 1.525761, Train acc: 68.602 %\n",
      "Epoch [16/20], Step [300/600], Train loss: 1.599801, Train acc: 69.056 %\n",
      "Epoch [16/20], Step [400/600], Train loss: 1.634498, Train acc: 69.272 %\n",
      "Epoch [16/20], Step [500/600], Train loss: 1.571588, Train acc: 69.349 %\n",
      "val_acc: 70.042 %, 0 / 10 (model updated!)\n",
      "Epoch [17/20], Step [0/600], Train loss: 1.619397, Train acc: 68.750 %\n",
      "Epoch [17/20], Step [100/600], Train loss: 1.657215, Train acc: 69.616 %\n",
      "Epoch [17/20], Step [200/600], Train loss: 1.522228, Train acc: 69.932 %\n",
      "Epoch [17/20], Step [300/600], Train loss: 1.512691, Train acc: 69.866 %\n",
      "Epoch [17/20], Step [400/600], Train loss: 1.570355, Train acc: 70.254 %\n",
      "Epoch [17/20], Step [500/600], Train loss: 1.582450, Train acc: 70.425 %\n",
      "val_acc: 71.073 %, 0 / 10 (model updated!)\n",
      "Epoch [18/20], Step [0/600], Train loss: 1.539133, Train acc: 73.438 %\n",
      "Epoch [18/20], Step [100/600], Train loss: 1.520334, Train acc: 70.730 %\n",
      "Epoch [18/20], Step [200/600], Train loss: 1.569953, Train acc: 71.183 %\n",
      "Epoch [18/20], Step [300/600], Train loss: 1.465243, Train acc: 71.076 %\n",
      "Epoch [18/20], Step [400/600], Train loss: 1.495702, Train acc: 71.478 %\n",
      "Epoch [18/20], Step [500/600], Train loss: 1.513605, Train acc: 71.423 %\n",
      "val_acc: 72.073 %, 0 / 10 (model updated!)\n",
      "Epoch [19/20], Step [0/600], Train loss: 1.467926, Train acc: 78.125 %\n",
      "Epoch [19/20], Step [100/600], Train loss: 1.504440, Train acc: 72.184 %\n",
      "Epoch [19/20], Step [200/600], Train loss: 1.478293, Train acc: 72.341 %\n",
      "Epoch [19/20], Step [300/600], Train loss: 1.475126, Train acc: 72.228 %\n",
      "Epoch [19/20], Step [400/600], Train loss: 1.470137, Train acc: 72.374 %\n",
      "Epoch [19/20], Step [500/600], Train loss: 1.393460, Train acc: 72.611 %\n",
      "val_acc: 73.010 %, 0 / 10 (model updated!)\n",
      "Epoch [20/20], Step [0/600], Train loss: 1.580185, Train acc: 60.938 %\n",
      "Epoch [20/20], Step [100/600], Train loss: 1.466332, Train acc: 72.386 %\n",
      "Epoch [20/20], Step [200/600], Train loss: 1.366286, Train acc: 73.002 %\n",
      "Epoch [20/20], Step [300/600], Train loss: 1.318331, Train acc: 72.981 %\n",
      "Epoch [20/20], Step [400/600], Train loss: 1.509999, Train acc: 72.923 %\n",
      "Epoch [20/20], Step [500/600], Train loss: 1.498143, Train acc: 73.091 %\n",
      "val_acc: 73.635 %, 0 / 10 (model updated!)\n",
      "Finish training\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# Training #\n",
    "############\n",
    "\n",
    "n_batch = len(train_loader)\n",
    "best_val_acc = 0\n",
    "esc = 0\n",
    "\n",
    "\n",
    "# Initialization \n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "# Start training \n",
    "for epoch in range(n_epoch):\n",
    "    correct_cnt, total_loss, total_cnt, train_loss, val_loss = 0, 0, 0, 0, 0\n",
    "    \n",
    "    for batch, (images, labels) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "        \n",
    "        predictions = model(images.view(-1, in_dim))\n",
    "        loss = loss_fcn(predictions, labels)\n",
    "    \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate the training loss and accuracy of each iteration\n",
    "        _, pred_labels = torch.max(predictions, 1)\n",
    "        total_cnt += images.size(0)\n",
    "        correct_cnt += (pred_labels == labels).sum().item()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Show the training information\n",
    "        if batch % 100 == 0 or batch == len(train_loader):\n",
    "            acc = correct_cnt / total_cnt\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{n_epoch}], Step [{batch}/{n_batch}], Train loss: {loss.item():.6f}, Train acc: {acc * 100:.3f} %\"\n",
    "            )\n",
    "    \n",
    "    # Validating\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():  # No need BP\n",
    "        total_cnt, correct_cnt = 0, 0\n",
    "        \n",
    "        for batch, (images, labels) in enumerate(val_loader, 1):\n",
    "            \n",
    "            # Put input tensor to GPU if it's available\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(images.view(-1, in_dim))\n",
    "            loss = loss_fcn(predictions, labels)\n",
    "            \n",
    "            # Calculate the training loss and accuracy of each iteration\n",
    "            _, pred_labels = torch.max(predictions, 1)\n",
    "            total_cnt += images.size(0)\n",
    "            correct_cnt += (pred_labels == labels).sum().item()\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        val_acc = correct_cnt / total_cnt\n",
    "        print(f\"val_acc: {val_acc * 100:.3f} %, {esc} / {MAX_ESC}\", end=' ')\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "            # Save trained model\n",
    "            torch.save(model.state_dict(), f\"./checkpoint/NN.pth\" )\n",
    "            print('(model updated!)')\n",
    "            esc = 0\n",
    "        else:\n",
    "            print('(model dropped)')\n",
    "            esc += 1\n",
    "\n",
    "        \n",
    "    if esc > MAX_ESC:\n",
    "        break\n",
    "\n",
    "model.train()\n",
    "\n",
    "print('Finish training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(dataloader, mode):\n",
    "    with torch.no_grad():  # No need BP\n",
    "        for batch, (images, labels) in enumerate(dataloader, 1):\n",
    "            \n",
    "            # Put input tensor to GPU if it's available\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(images.view(-1, in_dim))\n",
    "            loss = loss_fcn(predictions, labels)\n",
    "            \n",
    "            # Calculate the training loss and accuracy of each iteration\n",
    "            _, pred_labels = torch.max(predictions, 1)\n",
    "            total_cnt += images.size(0)\n",
    "            correct_cnt += (pred_labels == labels).sum().item()\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        val_acc = correct_cnt / total_cnt\n",
    "        print(f\"val_acc: {val_acc * 100:.3f} %, {esc} / {MAX_ESC}\", end=' ')\n",
    "        \n",
    "        if mode is 'validation':\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "\n",
    "                # Save trained model\n",
    "                torch.save(model.state_dict(), f\"./checkpoint/NN.pth\" )\n",
    "                print('(model updated!)')\n",
    "                esc = 0\n",
    "            else:\n",
    "                print('(model dropped)')\n",
    "                esc += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing accuracy of network:\t76.35 %\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# Testing #\n",
    "###########\n",
    "\n",
    "model.load_state_dict(torch.load(\"./checkpoint/NN.pth\"))\n",
    "model.cuda()\n",
    "\n",
    "with torch.no_grad(): # No need BP\n",
    "    \n",
    "    # Record variables and containers\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0] * 10\n",
    "    n_class_samples = [0] * 10\n",
    "    \n",
    "    # Loop through batches in test_loader\n",
    "    for images, labels in test_loader:\n",
    "        \n",
    "        # Get the GPU support\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "        \n",
    "        # Predict via forward pass\n",
    "        predictions = model(images.view(-1, 784))\n",
    "        _, pred_labels = torch.max(predictions, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (pred_labels == labels).sum().item()\n",
    "        \n",
    "        # Record correctness of each classes in this batch\n",
    "        for i in range(labels.size(0)):\n",
    "            label = labels[i]\n",
    "            pred = pred_labels[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "    \n",
    "    # Overall performance\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Testing accuracy of network:\\t{acc:.2f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of network:\t72.15 %\n38400\n3.727449655532837\n-0.008956225588917732\n1.2272506952285767\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model.load_state_dict(torch.load(\"./checkpoint/NN.pth\"))\n",
    "model = model.to(device)\n",
    "\n",
    "# Find threshold\n",
    "\n",
    "confidence_list = np.array([])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad(): # No need BP\n",
    "    \n",
    "    # Record variables and containers\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0] * 10\n",
    "    n_class_samples = [0] * 10\n",
    "    \n",
    "    # Loop through batches in test_loader\n",
    "    for images, labels in train_loader:\n",
    "        \n",
    "        # Get the GPU support\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "        \n",
    "        # Predict via forward pass\n",
    "        predictions = model(images.view(-1, 784))\n",
    "        confidence, predicted_labels = torch.max(predictions, dim=1)        \n",
    "        confidence_list = np.concatenate((confidence_list, confidence.cpu()), axis=0)\n",
    "        \n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted_labels == labels).sum().item()\n",
    "        \n",
    "        # Record correctness of each classes in this batch\n",
    "        for i in range(labels.size(0)):\n",
    "            label = labels[i]\n",
    "            pred = predicted_labels[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "    \n",
    "    # Overall performance\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of network:\\t{acc:.2f} %')\n",
    "\n",
    "print(len(confidence_list))\n",
    "print(confidence_list.max())\n",
    "print(confidence_list.min())\n",
    "threshold = np.sort(confidence_list)[int(len(confidence_list) / 2)]\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy of network:\t72.66 %\n12000\n5920\n5920\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "# Pseudo-labeling #\n",
    "###################\n",
    "\n",
    "import itertools\n",
    "\n",
    "confident_unlabels = []\n",
    "pseudo_labels = []\n",
    "\n",
    "# Load model\n",
    "model.load_state_dict(torch.load(\"./checkpoint/NN.pth\"))\n",
    "model = model.to(device)\n",
    "\n",
    "with torch.no_grad(): # No need BP\n",
    "    \n",
    "    # Record variables and containers\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0] * 10\n",
    "    n_class_samples = [0] * 10\n",
    "    \n",
    "    # Loop through batches in test_loader\n",
    "\n",
    "    # top = itertools.islice(unlabeled_loader, 5)\n",
    "    for images, labels in unlabeled_loader:\n",
    "        \n",
    "        # Get the GPU support\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "        \n",
    "        # Predict via forward pass\n",
    "        predictions = model(images.view(-1, 784))\n",
    "        \n",
    "        confidence, predicted_labels = torch.max(predictions, dim=1)\n",
    "        \n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted_labels == labels).sum().item()\n",
    "\n",
    "        for i, c in enumerate(confidence):\n",
    "            if c.item() > threshold:                \n",
    "                confident_unlabels.append(images[i])\n",
    "                pseudo_labels.append(predicted_labels[i])\n",
    "        \n",
    "        # Record correctness of each classes in this batch\n",
    "        for i in range(labels.size(0)):\n",
    "            label = labels[i]\n",
    "            pred = predicted_labels[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "    \n",
    "    # Overall performance\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of network:\\t{acc:.2f} %')\n",
    "\n",
    "print(unlabeled_size)\n",
    "print(len(confident_unlabels))\n",
    "print(len(pseudo_labels))\n",
    "confident_unlabels = torch.stack(confident_unlabels)\n",
    "pseudo_labels = torch.stack(pseudo_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([38400, 1, 28, 28])\ntorch.Size([38400])\n"
     ]
    }
   ],
   "source": [
    "# original_dataset = torch.empty(1, 28, 28)\n",
    "# original_labels = torch.empty((1,), dtype=torch.long)\n",
    "\n",
    "original_dataset = None\n",
    "original_labels = None\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    if original_dataset is None:\n",
    "        original_dataset = images\n",
    "    else:\n",
    "        original_dataset = torch.cat((original_dataset, images))\n",
    "    \n",
    "    if original_labels is None:\n",
    "        original_labels = labels\n",
    "    else:\n",
    "        original_labels = torch.cat((original_labels, labels))\n",
    "    \n",
    "    # print(images.shape)\n",
    "    # print(original_dataset.shape)\n",
    "    # print(labels.shape)\n",
    "    # original_dataset = torch.cat((original_dataset, images), 0)\n",
    "    # original_labels = torch.cat((original_labels, labels), 0)\n",
    "    # original_dataset.append(images)\n",
    "    # original_labels.append(labels)\n",
    "\n",
    "# original_dataset = torch.stack(original_dataset)\n",
    "# original_labels = torch.stack(original_labels)\n",
    "print(original_dataset.shape)\n",
    "print(original_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([5920, 1, 28, 28])\ntorch.Size([5920])\n"
     ]
    }
   ],
   "source": [
    "print(confident_unlabels.shape)\n",
    "print(pseudo_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "44320\n44320\n"
     ]
    }
   ],
   "source": [
    "new_dataset = torch.cat((original_dataset, confident_unlabels.cpu()))\n",
    "new_labels = torch.cat((original_labels, pseudo_labels.cpu()))\n",
    "print(len(new_dataset))\n",
    "print(len(new_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mydata(Dataset):\n",
    "    def __init__(self):\n",
    "        self.samples = new_dataset\n",
    "        self.labels = new_labels\n",
    "        self.n_samples = len(new_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # print(self.samples[index].unsqueeze(0).shape)\n",
    "        return self.samples[index], self.labels[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "new_data = mydata()\n",
    "new_loader = torch.utils.data.DataLoader(new_data, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch [1/20], Step [0/693], Train loss: 2.471171, Train acc: 3.125 %\n",
      "Epoch [1/20], Step [100/693], Train loss: 2.423783, Train acc: 3.311 %\n",
      "Epoch [1/20], Step [200/693], Train loss: 2.405116, Train acc: 3.234 %\n",
      "Epoch [1/20], Step [300/693], Train loss: 2.326942, Train acc: 3.364 %\n",
      "Epoch [1/20], Step [400/693], Train loss: 2.346068, Train acc: 3.671 %\n",
      "Epoch [1/20], Step [500/693], Train loss: 2.366001, Train acc: 4.023 %\n",
      "Epoch [1/20], Step [600/693], Train loss: 2.357221, Train acc: 4.352 %\n",
      "val_acc: 5.045 %, 0 / 10 (model updated!)\n",
      "Epoch [2/20], Step [0/693], Train loss: 2.341733, Train acc: 7.812 %\n",
      "Epoch [2/20], Step [100/693], Train loss: 2.345186, Train acc: 7.596 %\n",
      "Epoch [2/20], Step [200/693], Train loss: 2.269467, Train acc: 8.388 %\n",
      "Epoch [2/20], Step [300/693], Train loss: 2.273064, Train acc: 8.768 %\n",
      "Epoch [2/20], Step [400/693], Train loss: 2.297820, Train acc: 9.348 %\n",
      "Epoch [2/20], Step [500/693], Train loss: 2.292525, Train acc: 9.858 %\n",
      "Epoch [2/20], Step [600/693], Train loss: 2.295155, Train acc: 10.542 %\n",
      "val_acc: 12.064 %, 0 / 10 (model updated!)\n",
      "Epoch [3/20], Step [0/693], Train loss: 2.255055, Train acc: 10.938 %\n",
      "Epoch [3/20], Step [100/693], Train loss: 2.251648, Train acc: 17.342 %\n",
      "Epoch [3/20], Step [200/693], Train loss: 2.175693, Train acc: 17.491 %\n",
      "Epoch [3/20], Step [300/693], Train loss: 2.211716, Train acc: 17.935 %\n",
      "Epoch [3/20], Step [400/693], Train loss: 2.227703, Train acc: 18.731 %\n",
      "Epoch [3/20], Step [500/693], Train loss: 2.182098, Train acc: 19.271 %\n",
      "Epoch [3/20], Step [600/693], Train loss: 2.115119, Train acc: 20.003 %\n",
      "val_acc: 21.317 %, 0 / 10 (model updated!)\n",
      "Epoch [4/20], Step [0/693], Train loss: 2.109391, Train acc: 26.562 %\n",
      "Epoch [4/20], Step [100/693], Train loss: 2.150393, Train acc: 26.516 %\n",
      "Epoch [4/20], Step [200/693], Train loss: 2.179885, Train acc: 26.562 %\n",
      "Epoch [4/20], Step [300/693], Train loss: 2.114978, Train acc: 27.450 %\n",
      "Epoch [4/20], Step [400/693], Train loss: 2.110216, Train acc: 27.798 %\n",
      "Epoch [4/20], Step [500/693], Train loss: 2.029249, Train acc: 28.633 %\n",
      "Epoch [4/20], Step [600/693], Train loss: 2.086971, Train acc: 29.077 %\n",
      "val_acc: 30.074 %, 0 / 10 (model updated!)\n",
      "Epoch [5/20], Step [0/693], Train loss: 2.136365, Train acc: 32.812 %\n",
      "Epoch [5/20], Step [100/693], Train loss: 2.102968, Train acc: 34.236 %\n",
      "Epoch [5/20], Step [200/693], Train loss: 1.988895, Train acc: 34.818 %\n",
      "Epoch [5/20], Step [300/693], Train loss: 2.062371, Train acc: 35.444 %\n",
      "Epoch [5/20], Step [400/693], Train loss: 1.903400, Train acc: 36.179 %\n",
      "Epoch [5/20], Step [500/693], Train loss: 1.974934, Train acc: 36.742 %\n",
      "Epoch [5/20], Step [600/693], Train loss: 1.988881, Train acc: 37.386 %\n",
      "val_acc: 38.286 %, 0 / 10 (model updated!)\n",
      "Epoch [6/20], Step [0/693], Train loss: 2.053728, Train acc: 31.250 %\n",
      "Epoch [6/20], Step [100/693], Train loss: 1.974121, Train acc: 41.584 %\n",
      "Epoch [6/20], Step [200/693], Train loss: 1.919949, Train acc: 43.004 %\n",
      "Epoch [6/20], Step [300/693], Train loss: 1.953794, Train acc: 43.714 %\n",
      "Epoch [6/20], Step [400/693], Train loss: 1.861434, Train acc: 43.992 %\n",
      "Epoch [6/20], Step [500/693], Train loss: 1.838538, Train acc: 44.770 %\n",
      "Epoch [6/20], Step [600/693], Train loss: 2.009011, Train acc: 45.357 %\n",
      "val_acc: 45.979 %, 0 / 10 (model updated!)\n",
      "Epoch [7/20], Step [0/693], Train loss: 1.930308, Train acc: 48.438 %\n",
      "Epoch [7/20], Step [100/693], Train loss: 1.926943, Train acc: 49.830 %\n",
      "Epoch [7/20], Step [200/693], Train loss: 1.905961, Train acc: 50.723 %\n",
      "Epoch [7/20], Step [300/693], Train loss: 1.883375, Train acc: 51.012 %\n",
      "Epoch [7/20], Step [400/693], Train loss: 1.943609, Train acc: 51.161 %\n",
      "Epoch [7/20], Step [500/693], Train loss: 1.807009, Train acc: 51.853 %\n",
      "Epoch [7/20], Step [600/693], Train loss: 1.899615, Train acc: 52.407 %\n",
      "val_acc: 52.765 %, 0 / 10 (model updated!)\n",
      "Epoch [8/20], Step [0/693], Train loss: 1.853994, Train acc: 57.812 %\n",
      "Epoch [8/20], Step [100/693], Train loss: 1.804711, Train acc: 56.033 %\n",
      "Epoch [8/20], Step [200/693], Train loss: 1.775497, Train acc: 56.584 %\n",
      "Epoch [8/20], Step [300/693], Train loss: 1.826767, Train acc: 56.821 %\n",
      "Epoch [8/20], Step [400/693], Train loss: 1.833282, Train acc: 57.310 %\n",
      "Epoch [8/20], Step [500/693], Train loss: 1.850497, Train acc: 57.566 %\n",
      "Epoch [8/20], Step [600/693], Train loss: 1.819638, Train acc: 57.605 %\n",
      "val_acc: 57.645 %, 0 / 10 (model updated!)\n",
      "Epoch [9/20], Step [0/693], Train loss: 1.871743, Train acc: 48.438 %\n",
      "Epoch [9/20], Step [100/693], Train loss: 1.789018, Train acc: 59.127 %\n",
      "Epoch [9/20], Step [200/693], Train loss: 1.722682, Train acc: 60.067 %\n",
      "Epoch [9/20], Step [300/693], Train loss: 1.716297, Train acc: 60.538 %\n",
      "Epoch [9/20], Step [400/693], Train loss: 1.782781, Train acc: 60.661 %\n",
      "Epoch [9/20], Step [500/693], Train loss: 1.732999, Train acc: 60.928 %\n",
      "Epoch [9/20], Step [600/693], Train loss: 1.804041, Train acc: 61.130 %\n",
      "val_acc: 61.098 %, 0 / 10 (model updated!)\n",
      "Epoch [10/20], Step [0/693], Train loss: 1.712915, Train acc: 59.375 %\n",
      "Epoch [10/20], Step [100/693], Train loss: 1.784831, Train acc: 62.485 %\n",
      "Epoch [10/20], Step [200/693], Train loss: 1.778647, Train acc: 62.788 %\n",
      "Epoch [10/20], Step [300/693], Train loss: 1.590647, Train acc: 62.853 %\n",
      "Epoch [10/20], Step [400/693], Train loss: 1.720134, Train acc: 63.260 %\n",
      "Epoch [10/20], Step [500/693], Train loss: 1.589056, Train acc: 63.470 %\n",
      "Epoch [10/20], Step [600/693], Train loss: 1.678103, Train acc: 63.582 %\n",
      "val_acc: 63.563 %, 0 / 10 (model updated!)\n",
      "Epoch [11/20], Step [0/693], Train loss: 1.746496, Train acc: 54.688 %\n",
      "Epoch [11/20], Step [100/693], Train loss: 1.713075, Train acc: 64.511 %\n",
      "Epoch [11/20], Step [200/693], Train loss: 1.601744, Train acc: 64.855 %\n",
      "Epoch [11/20], Step [300/693], Train loss: 1.545736, Train acc: 65.329 %\n",
      "Epoch [11/20], Step [400/693], Train loss: 1.666886, Train acc: 65.430 %\n",
      "Epoch [11/20], Step [500/693], Train loss: 1.650988, Train acc: 65.800 %\n",
      "Epoch [11/20], Step [600/693], Train loss: 1.644835, Train acc: 65.955 %\n",
      "val_acc: 65.786 %, 0 / 10 (model updated!)\n",
      "Epoch [12/20], Step [0/693], Train loss: 1.620078, Train acc: 64.062 %\n",
      "Epoch [12/20], Step [100/693], Train loss: 1.640451, Train acc: 66.785 %\n",
      "Epoch [12/20], Step [200/693], Train loss: 1.624550, Train acc: 67.568 %\n",
      "Epoch [12/20], Step [300/693], Train loss: 1.514579, Train acc: 67.681 %\n",
      "Epoch [12/20], Step [400/693], Train loss: 1.626720, Train acc: 67.655 %\n",
      "Epoch [12/20], Step [500/693], Train loss: 1.528555, Train acc: 67.886 %\n",
      "Epoch [12/20], Step [600/693], Train loss: 1.558074, Train acc: 67.752 %\n",
      "val_acc: 67.632 %, 0 / 10 (model updated!)\n",
      "Epoch [13/20], Step [0/693], Train loss: 1.621125, Train acc: 59.375 %\n",
      "Epoch [13/20], Step [100/693], Train loss: 1.563516, Train acc: 68.889 %\n",
      "Epoch [13/20], Step [200/693], Train loss: 1.557876, Train acc: 68.828 %\n",
      "Epoch [13/20], Step [300/693], Train loss: 1.504600, Train acc: 68.854 %\n",
      "Epoch [13/20], Step [400/693], Train loss: 1.503581, Train acc: 69.003 %\n",
      "Epoch [13/20], Step [500/693], Train loss: 1.452858, Train acc: 69.308 %\n",
      "Epoch [13/20], Step [600/693], Train loss: 1.610103, Train acc: 69.494 %\n",
      "val_acc: 69.266 %, 0 / 10 (model updated!)\n",
      "Epoch [14/20], Step [0/693], Train loss: 1.487155, Train acc: 75.000 %\n",
      "Epoch [14/20], Step [100/693], Train loss: 1.501389, Train acc: 70.931 %\n",
      "Epoch [14/20], Step [200/693], Train loss: 1.560962, Train acc: 71.043 %\n",
      "Epoch [14/20], Step [300/693], Train loss: 1.535982, Train acc: 70.816 %\n",
      "Epoch [14/20], Step [400/693], Train loss: 1.429656, Train acc: 70.683 %\n",
      "Epoch [14/20], Step [500/693], Train loss: 1.571289, Train acc: 70.774 %\n",
      "Epoch [14/20], Step [600/693], Train loss: 1.445171, Train acc: 70.845 %\n",
      "val_acc: 70.605 %, 0 / 10 (model updated!)\n",
      "Epoch [15/20], Step [0/693], Train loss: 1.458920, Train acc: 75.000 %\n",
      "Epoch [15/20], Step [100/693], Train loss: 1.556083, Train acc: 70.931 %\n",
      "Epoch [15/20], Step [200/693], Train loss: 1.454925, Train acc: 71.074 %\n",
      "Epoch [15/20], Step [300/693], Train loss: 1.469984, Train acc: 71.294 %\n",
      "Epoch [15/20], Step [400/693], Train loss: 1.567114, Train acc: 71.622 %\n",
      "Epoch [15/20], Step [500/693], Train loss: 1.464831, Train acc: 71.694 %\n",
      "Epoch [15/20], Step [600/693], Train loss: 1.466745, Train acc: 71.914 %\n",
      "val_acc: 71.853 %, 0 / 10 (model updated!)\n",
      "Epoch [16/20], Step [0/693], Train loss: 1.448712, Train acc: 71.875 %\n",
      "Epoch [16/20], Step [100/693], Train loss: 1.514900, Train acc: 72.401 %\n",
      "Epoch [16/20], Step [200/693], Train loss: 1.401726, Train acc: 72.808 %\n",
      "Epoch [16/20], Step [300/693], Train loss: 1.449278, Train acc: 73.162 %\n",
      "Epoch [16/20], Step [400/693], Train loss: 1.341185, Train acc: 73.137 %\n",
      "Epoch [16/20], Step [500/693], Train loss: 1.345583, Train acc: 73.048 %\n",
      "Epoch [16/20], Step [600/693], Train loss: 1.346195, Train acc: 73.022 %\n",
      "val_acc: 73.003 %, 0 / 10 (model updated!)\n",
      "Epoch [17/20], Step [0/693], Train loss: 1.362746, Train acc: 76.562 %\n",
      "Epoch [17/20], Step [100/693], Train loss: 1.413240, Train acc: 74.288 %\n",
      "Epoch [17/20], Step [200/693], Train loss: 1.420226, Train acc: 74.331 %\n",
      "Epoch [17/20], Step [300/693], Train loss: 1.379590, Train acc: 74.294 %\n",
      "Epoch [17/20], Step [400/693], Train loss: 1.418349, Train acc: 74.205 %\n",
      "Epoch [17/20], Step [500/693], Train loss: 1.397844, Train acc: 74.046 %\n",
      "Epoch [17/20], Step [600/693], Train loss: 1.388621, Train acc: 74.054 %\n",
      "val_acc: 73.978 %, 0 / 10 (model updated!)\n",
      "Epoch [18/20], Step [0/693], Train loss: 1.320461, Train acc: 75.000 %\n",
      "Epoch [18/20], Step [100/693], Train loss: 1.338750, Train acc: 74.041 %\n",
      "Epoch [18/20], Step [200/693], Train loss: 1.259375, Train acc: 74.635 %\n",
      "Epoch [18/20], Step [300/693], Train loss: 1.160779, Train acc: 74.548 %\n",
      "Epoch [18/20], Step [400/693], Train loss: 1.402558, Train acc: 74.606 %\n",
      "Epoch [18/20], Step [500/693], Train loss: 1.360293, Train acc: 74.838 %\n",
      "Epoch [18/20], Step [600/693], Train loss: 1.230632, Train acc: 75.044 %\n",
      "val_acc: 74.905 %, 0 / 10 (model updated!)\n",
      "Epoch [19/20], Step [0/693], Train loss: 1.325188, Train acc: 75.000 %\n",
      "Epoch [19/20], Step [100/693], Train loss: 1.274049, Train acc: 76.702 %\n",
      "Epoch [19/20], Step [200/693], Train loss: 1.263001, Train acc: 76.391 %\n",
      "Epoch [19/20], Step [300/693], Train loss: 1.254711, Train acc: 76.100 %\n",
      "Epoch [19/20], Step [400/693], Train loss: 1.254533, Train acc: 75.970 %\n",
      "Epoch [19/20], Step [500/693], Train loss: 1.347210, Train acc: 76.029 %\n",
      "Epoch [19/20], Step [600/693], Train loss: 1.349738, Train acc: 75.975 %\n",
      "val_acc: 75.746 %, 0 / 10 (model updated!)\n",
      "Epoch [20/20], Step [0/693], Train loss: 1.270822, Train acc: 68.750 %\n",
      "Epoch [20/20], Step [100/693], Train loss: 1.273749, Train acc: 76.578 %\n",
      "Epoch [20/20], Step [200/693], Train loss: 1.286145, Train acc: 76.803 %\n",
      "Epoch [20/20], Step [300/693], Train loss: 1.214944, Train acc: 76.926 %\n",
      "Epoch [20/20], Step [400/693], Train loss: 1.289489, Train acc: 76.956 %\n",
      "Epoch [20/20], Step [500/693], Train loss: 1.337812, Train acc: 76.821 %\n",
      "Epoch [20/20], Step [600/693], Train loss: 1.242595, Train acc: 76.679 %\n",
      "val_acc: 76.482 %, 0 / 10 (model updated!)\n",
      "Finish training\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# Retrain #\n",
    "###########\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model.load_state_dict(torch.load(\"./checkpoint/NN.pth\"))\n",
    "\n",
    "n_batch = len(new_loader)\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "############\n",
    "# Training #\n",
    "############\n",
    "best_val_acc = 0\n",
    "esc = 0\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    correct_cnt, total_loss, total_cnt, train_loss, val_loss = 0, 0, 0, 0, 0\n",
    "    \n",
    "    for batch, (images, labels) in enumerate(new_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "        \n",
    "        predictions = model(images.view(-1, in_dim))\n",
    "        loss = loss_fcn(predictions, labels)\n",
    "    \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate the training loss and accuracy of each iteration\n",
    "        _, pred_labels = torch.max(predictions, 1)\n",
    "        total_cnt += images.size(0)\n",
    "        correct_cnt += (pred_labels == labels).sum().item()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Show the training information\n",
    "        if batch % 100 == 0 or batch == len(new_loader):\n",
    "            acc = correct_cnt / total_cnt\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{n_epoch}], Step [{batch}/{n_batch}], Train loss: {loss.item():.6f}, Train acc: {acc * 100:.3f} %\"\n",
    "            )\n",
    "    \n",
    "    ##############\n",
    "    # Validating #\n",
    "    ##############\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():  # No need BP\n",
    "        for batch, (images, labels) in enumerate(val_loader, 1):\n",
    "            \n",
    "            # Put input tensor to GPU if it's available\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(images.view(-1, in_dim))\n",
    "            loss = loss_fcn(predictions, labels)\n",
    "            \n",
    "            # Calculate the training loss and accuracy of each iteration\n",
    "            _, pred_labels = torch.max(predictions, 1)\n",
    "            total_cnt += images.size(0)\n",
    "            correct_cnt += (pred_labels == labels).sum().item()\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        val_acc = correct_cnt / total_cnt\n",
    "        print(f\"val_acc: {val_acc * 100:.3f} %, {esc} / {MAX_ESC}\", end=' ')\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "            # Save trained model\n",
    "            torch.save(model.state_dict(), f\"./checkpoint/NN.pth\" )\n",
    "            print('(model updated!)')\n",
    "            esc = 0\n",
    "        else:\n",
    "            print('(model dropped)')\n",
    "            esc += 1\n",
    "\n",
    "        \n",
    "    if esc > MAX_ESC:\n",
    "        break\n",
    "\n",
    "model.train()\n",
    "\n",
    "print('Finish training')"
   ]
  }
 ]
}